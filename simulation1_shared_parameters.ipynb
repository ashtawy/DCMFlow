{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import DCMFlow_094 as dcm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import random\n",
    "import operator\n",
    "import random as rn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import collections\n",
    "from scipy.stats import truncnorm\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate nested logit tree and utility expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_nltree(strct=None):\n",
    "    n_levels = len(strct)\n",
    "    nltree_dic = {'l0':'Root', 'l1':['L1C' + str(n) for n in range(1, strct[0]+1)]}\n",
    "    for level in range(1, n_levels):\n",
    "        level_nests = []\n",
    "        branch_cntr = 1\n",
    "        for p_node in nltree_dic['l'+str(level)]:\n",
    "            for j in range(1, strct[level]+1):\n",
    "                j_node = p_node + '_' + 'L%dC%d'%(level+1, branch_cntr)\n",
    "                level_nests.append(j_node)\n",
    "                branch_cntr += 1\n",
    "        nltree_dic['l'+str(level+1)] = level_nests\n",
    "    return nltree_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_choices2covariates_map(n_choices, n_covariates):\n",
    "    max_combs = 2**n_covariates\n",
    "    d = np.random.choice(max_combs, max_combs, replace=False)\n",
    "    cov2chc_map = np.zeros(shape=[n_covariates, n_choices], dtype=int)\n",
    "    colum = 0\n",
    "    mn_mx_n_covs = 0\n",
    "    for c in range(max_combs):\n",
    "        fv = list(map(int, list(np.binary_repr(d[c], width=n_covariates))))\n",
    "        c_nftrs = sum(fv)\n",
    "        if c_nftrs > mn_mx_n_covs and c_nftrs <= n_covariates:\n",
    "            cov2chc_map[:, colum] = fv\n",
    "            colum += 1\n",
    "        if colum==n_choices:\n",
    "            break\n",
    "    assert colum==n_choices\n",
    "    return np.transpose(cov2chc_map*np.arange(1, n_covariates+1)[:, None])\n",
    "\n",
    "def create_expression(a, pnames):\n",
    "    non_zeros = []\n",
    "    for i, ai in enumerate(a):\n",
    "        if ai != 0:\n",
    "            pname = pnames[i]\n",
    "            xname = '' if pname[0]=='A' else '*X'+str(i+1)\n",
    "            non_zeros.append(pname+xname)\n",
    "    return '+'.join(non_zeros)\n",
    "\n",
    "def generate_random_utilities(nltree_dict, n_covariates):\n",
    "    n_levels = len(nltree_dict) - 1\n",
    "    choices = nltree_dict['l'+str(n_levels)]\n",
    "    n_choices = len(choices)\n",
    "    beta_map = generate_random_choices2covariates_map(n_choices, n_covariates)\n",
    "    alpha_map = beta_map.copy()\n",
    "    alpha_map[alpha_map > 0] = 1\n",
    "    uts = collections.OrderedDict()\n",
    "    coefs_indices = np.unique(beta_map)\n",
    "    n_coefficients = len(coefs_indices[coefs_indices>0])\n",
    "    coefficient_names = ['B'+str(c) for c in range(1, n_coefficients+1)]\n",
    "    const_names = ['A'+str(c) for c in range(1, alpha_map.shape[1]+1)]\n",
    "    for choice in range(n_choices):\n",
    "        ut_eq_alpha = create_expression(alpha_map[choice,:], const_names)\n",
    "        ut_eq_beta = create_expression(beta_map[choice,:], coefficient_names)\n",
    "        uts[choices[choice]] = '+'.join([ut_eq_alpha, ut_eq_beta])\n",
    "    return uts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to generate random covariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(nltree_dict, n_cases, \n",
    "                  n_covariates, \n",
    "                  avail_rate_mean=0.1, \n",
    "                  avail_rate_sd=0.05,\n",
    "                  covs_means_mn=0.5,\n",
    "                  covs_means_mx=1.5,\n",
    "                  covs_std_from_means=0.1,\n",
    "                  use_choice_ids=True):\n",
    "    \n",
    "    n_levels = len(nltree_dict) - 1\n",
    "    choices = nltree_dict['l' + str(n_levels)]\n",
    "    n_choices = len(choices)\n",
    "    print('n_choices = %d'%(n_choices))\n",
    "    def gen_choices(n_avail_choices):\n",
    "        n_avail_choices = min(n_choices, n_avail_choices)\n",
    "        if use_choice_ids:\n",
    "            avail_choices = list(np.sort(np.random.choice(choices, n_avail_choices, replace=False)))\n",
    "        else:\n",
    "            avail_choices = list(np.sort(np.random.choice(n_choices, n_avail_choices, replace=False)))\n",
    "        return avail_choices\n",
    "    \n",
    "    loc = avail_rate_mean*n_choices\n",
    "    scale = avail_rate_sd*n_choices\n",
    "    n_avail_choices = truncnorm(a=2, b=n_choices, loc=loc, scale=scale).rvs(size=n_cases).astype(int)\n",
    "    choice_ids_list = list(map(gen_choices, n_avail_choices))\n",
    "    case_ids = []\n",
    "    choice_ids = []\n",
    "    for caseid in range(len(choice_ids_list)):\n",
    "        sub_l = choice_ids_list[caseid]\n",
    "        case_ids.extend([caseid]*len(sub_l))\n",
    "        choice_ids.extend(sub_l)\n",
    "    ncases_times_n_avail_choices = len(choice_ids)\n",
    "    covs_means = np.random.uniform(covs_means_mn, covs_means_mx, n_covariates)\n",
    "    covs = np.random.normal(loc=covs_means, scale=covs_std_from_means*covs_means, \n",
    "                            size=[ncases_times_n_avail_choices, n_covariates])\n",
    "    cov_names = ['X%d'%(i+1) for i in range(n_covariates)]\n",
    "    d_pd = pd.DataFrame(data=covs, columns = cov_names)\n",
    "    d_pd['caseid'] = case_ids\n",
    "    d_pd['choiceid'] = choice_ids\n",
    "    return d_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate random parameter values and constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_random_parameters(nltree_dict, n_covariates=None, theta_bins=None, \n",
    "                          theta_mn=0.65, theta_mx=0.95):\n",
    "    n_levels = len(nltree_dict) - 1\n",
    "    bins = theta_bins\n",
    "    if bins is None:\n",
    "        bnds = np.linspace(start=theta_mn, stop=theta_mx, num=n_levels)\n",
    "        bins = [[bnds[i], None] \n",
    "                if i < n_levels-2 else \n",
    "                [bnds[i], bnds[i+1]] \n",
    "                for i in range(n_levels-1)][::-1]\n",
    "    params_dict = {}\n",
    "    logsum_level = []\n",
    "    for node_name in nltree_dict['l1']:\n",
    "        logsum = np.random.uniform(bins[0][0], bins[0][1])\n",
    "        params_dict[node_name] = logsum\n",
    "        logsum_level.append(logsum)\n",
    "    logsum_levels = [copy.deepcopy(logsum_level)]\n",
    "    for level in range(1, n_levels-1):\n",
    "        logsum_level = []\n",
    "        for i, p_node in enumerate(nltree_dict['l'+str(level)]):\n",
    "            for j, c_node in enumerate(nltree_dict['l'+str(level+1)]):\n",
    "                if p_node in c_node:\n",
    "                    logsum = np.random.uniform(bins[level][0], logsum_levels[level-1][i])\n",
    "                    params_dict[c_node] = logsum\n",
    "                    logsum_level.append(logsum)\n",
    "            logsum_levels.append(logsum_level)\n",
    "    n_choices = len(nltree_dict['l' + str(n_levels)])\n",
    "    n_covariates = n_choices if n_covariates is None else n_covariates\n",
    "    for i in range(n_covariates):\n",
    "        params_dict['A%d'%(i+1)] = np.round(np.random.uniform(1.0, 2.0), 2) \n",
    "        params_dict['B%d'%(i+1)] = np.round(np.random.uniform(-2.0, -0.1), 2) \n",
    "    return params_dict\n",
    "\n",
    "def get_logsum_constraints(nltree_dict):\n",
    "    n_levels = len(nltree_dict) - 1\n",
    "    constraints = {}\n",
    "    for level in range(1, n_levels):\n",
    "        nests = nltree_dict['l%d'%(level)]\n",
    "        for nest in nests:\n",
    "            constraints[nest] = (0, 1.0)\n",
    "    return constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_covariates = 20\n",
    "nltree_dict = generate_nltree(strct=[3,4,5,10])\n",
    "#nltree_dic = generate_nltree(strct=[2,3,4])\n",
    "utilities_dict = generate_random_utilities(nltree_dict, n_covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_choices = 600\n"
     ]
    }
   ],
   "source": [
    "n_cases = 80000\n",
    "data = generate_data(nltree_dict, n_cases=n_cases, n_covariates=n_covariates, \n",
    "                     avail_rate_mean=0.5, avail_rate_sd=0.1,\n",
    "                     covs_std_from_means=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_params_dict = generate_random_parameters(nltree_dict, n_covariates=n_covariates, \n",
    "                                         theta_mn=0.6, theta_mx=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an NLFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = dcm.NLFlow(nltree_dict=nltree_dict, utilities_dict=utilities_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate choices & compute loglikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loglikelihood (sum/mean) = 298029.0/3.725\n"
     ]
    }
   ],
   "source": [
    "choices_and_ll = m.compute_choices_and_likelihood(data, true_params_dict)\n",
    "if choices_and_ll is not None:\n",
    "    simulated_choices, ll = choices_and_ll\n",
    "    data_with_y = data.copy()\n",
    "    data_with_y['chosen'] = simulated_choices['chosen'].copy()\n",
    "    print('Loglikelihood (sum/mean) = %.1f/%.3f'%(-ll, -ll/n_cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the model's parameters using the L-BFGS-B optimizer (and compare them with true values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digesting the data...\n",
      "\n",
      "Starting the optimization process using l-bfgs-b\n"
     ]
    }
   ],
   "source": [
    "constraints_dict = get_logsum_constraints(nltree_dict)\n",
    "m.fit(data_with_y, optimizer='l-bfgs-b',\n",
    "      true_params_dict=true_params_dict, \n",
    "      constraints_dict=constraints_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print estimated and true parameters side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Model's parameters have not been estimated yet. Run 'fit()' first to estimate them.\n"
     ]
    }
   ],
   "source": [
    "m.get_estimated_parameters(true_params_dict=true_params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You could employ Adam to optimize using stochastic-gradient descent"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "You could use m.fit(data_with_y, optimizer='adam', ...), but that would run Adam in batch mode where at each\n",
    "iteration the entire data will be used to update the parameters. To run Adam on smaller samples of the data \n",
    "(mini-batches), you will need create and AdamOptimizer object and set its step_size to the desired value,  \n",
    "E.g., below I use 1000 cases, I can also set other hyper-parameters as shown below and the left (not listed)\n",
    "assigned to their default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = dcm.AdamOptimizer(n_steps=25000, log_every_n_epochs=1, \n",
    "                        step_size=1000, learning_rate=2e-4, \n",
    "                        patience=2000, epsilon=1e-8,\n",
    "                        interior_point_penalty_term=0.0005,\n",
    "                        objective='loglikelihood_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digesting the data...\n",
      "\n",
      "Starting the optimization process using adam\n",
      "\n",
      "\n",
      "Abbreviations key:\n",
      "S: Step, E: Epoch, L: Loglikelihood, S/M: Sum/Mean\n",
      "C: Cost (loglikehood + penalty terms for constrained \n",
      "   parameters, printed when constraints are used)\n",
      "T: Time, Cf/Ls/Cn: Coefficient/Logsum/Constant\n",
      "MAPE: Mean Absolute Percentage Error (printed when true\n",
      "      parameters are provided)\n",
      "RMSE: Root Mean Square Error (printed when true parameters\n",
      "      are provided)\n",
      "\n",
      "\n",
      "S/E=80/1, L(S/M)=481151.8/6.014, C(S/M)=481159.0/6.014, T=212s, Cf/Ls/Cn MAPE=102.1/30.9/99.4%, RMSE=1.134/0.214/1.420\n",
      "S/E=160/2, L(S/M)=469714.7/5.871, C(S/M)=469721.8/5.872, T=423s, Cf/Ls/Cn MAPE=104.1/30.1/98.7%, RMSE=1.138/0.210/1.409\n",
      "S/E=240/3, L(S/M)=459411.0/5.743, C(S/M)=459418.0/5.743, T=631s, Cf/Ls/Cn MAPE=106.1/29.2/98.0%, RMSE=1.142/0.205/1.399\n",
      "S/E=320/4, L(S/M)=450108.1/5.626, C(S/M)=450115.1/5.626, T=841s, Cf/Ls/Cn MAPE=108.0/28.2/97.3%, RMSE=1.146/0.201/1.389\n"
     ]
    }
   ],
   "source": [
    "m.fit(data_with_y, optimizer=opt, \n",
    "      true_params_dict=true_params_dict,\n",
    "      constraints_dict=constraints_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The optimization failed here, consider increasing the value of interior_point_penalty_term and/or the values of other hyper-parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
